SubmissionNumber#=%=#31
FinalPaperTitle#=%=#Recent adventures with emotion-reading technology
ShortPaperTitle#=%=#Recent adventures with emotion-reading technology
Author{1}{Lastname}#=%=#Picard
Author{1}{Firstname}#=%=#Rosalind
Author{1}{Email}#=%=#picard@media.mit.edu
Author{1}{Affiliation}#=%=#MIT Media Labs
Author{2}{Lastname}#=%=#
Author{2}{Firstname}#=%=#
Author{2}{Email}#=%=#
Author{2}{Affiliation}#=%=#
Author{3}{Lastname}#=%=#
Author{3}{Firstname}#=%=#
Author{3}{Email}#=%=#
Author{3}{Affiliation}#=%=#
Author{4}{Lastname}#=%=#
Author{4}{Firstname}#=%=#
Author{4}{Email}#=%=#
Author{4}{Affiliation}#=%=#
Author{5}{Lastname}#=%=#
Author{5}{Firstname}#=%=#
Author{5}{Email}#=%=#
Author{5}{Affiliation}#=%=#
Author{6}{Lastname}#=%=#
Author{6}{Firstname}#=%=#
Author{6}{Email}#=%=#
Author{6}{Affiliation}#=%=#
Author{7}{Lastname}#=%=#
Author{7}{Firstname}#=%=#
Author{7}{Email}#=%=#
Author{7}{Affiliation}#=%=#
Author{8}{Lastname}#=%=#
Author{8}{Firstname}#=%=#
Author{8}{Email}#=%=#
Author{8}{Affiliation}#=%=#
Author{9}{Lastname}#=%=#
Author{9}{Firstname}#=%=#
Author{9}{Email}#=%=#
Author{9}{Affiliation}#=%=#
Author{10}{Lastname}#=%=#
Author{10}{Firstname}#=%=#
Author{10}{Email}#=%=#
Author{10}{Affiliation}#=%=#
Author{11}{Lastname}#=%=#
Author{11}{Firstname}#=%=#
Author{11}{Email}#=%=#
Author{11}{Affiliation}#=%=#
Author{12}{Lastname}#=%=#
Author{12}{Firstname}#=%=#
Author{12}{Email}#=%=#
Author{12}{Affiliation}#=%=#
Author{13}{Lastname}#=%=#
Author{13}{Firstname}#=%=#
Author{13}{Email}#=%=#
Author{13}{Affiliation}#=%=#
Author{14}{Lastname}#=%=#
Author{14}{Firstname}#=%=#
Author{14}{Email}#=%=#
Author{14}{Affiliation}#=%=#
Author{15}{Lastname}#=%=#
Author{15}{Firstname}#=%=#
Author{15}{Email}#=%=#
Author{15}{Affiliation}#=%=#
NumberOfPages#=%=#1
CopyrightSigned#=%=#Rosalind Picard
JobTitle#==#Researcher
Organization#==#MIT Media Labs
Abstract#==#This talk will share stories from recent investigations at the MIT Media Lab in
creating technology to recognize and better communicate emotion.  Examples
include automating facial affect recognition online for sharing media
experiences, gathering the world’s largest sets of natural expressions
(instead of lab-elicited data) and training machine learning models to predict
liking of the experience based on expression dynamics throughout the
experience.   We also have found that most people have difficulty
discriminating “peak smiles of frustration” from “peak smiles of
delight” in static images.  With machine learning and dynamic features, we
were able to teach the computer to be highly accurate at discriminating these. 
These kinds of tools can potentially help many people with nonverbal learning
disabilities, limited vision, social phobia, or autism who find it challenging
to read the faces of those around them.  I will also share recent findings from
people wearing physiological sensors 24/7, and how we’ve been learning about
connections between the emotion system, sleep and seizures.  Finally, I will
share some of our newest work related to crowd sourcing cognitive-behavioral
therapy and computational empathy, where sentiment analysis could be of huge
benefit.
==========