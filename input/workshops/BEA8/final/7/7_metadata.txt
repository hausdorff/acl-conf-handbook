SubmissionNumber#=%=#7
FinalPaperTitle#=%=#Automated Content Scoring of Spoken Responses in an Assessment for Teachers of English
ShortPaperTitle#=%=#Automated Content Scoring of Spoken Responses in an Assessment for Teachers of English
Author{1}{Lastname}#=%=#Zechner
Author{1}{Firstname}#=%=#Klaus
Author{1}{Email}#=%=#KZechner@ETS.ORG
Author{1}{Affiliation}#=%=#Educational Testing Service
Author{2}{Lastname}#=%=#Wang
Author{2}{Firstname}#=%=#Xinhao
Author{2}{Email}#=%=#xwang002@ets.org
Author{2}{Affiliation}#=%=#Educational Testing Service
Author{3}{Lastname}#=%=#
Author{3}{Firstname}#=%=#
Author{3}{Email}#=%=#
Author{3}{Affiliation}#=%=#
Author{4}{Lastname}#=%=#
Author{4}{Firstname}#=%=#
Author{4}{Email}#=%=#
Author{4}{Affiliation}#=%=#
Author{5}{Lastname}#=%=#
Author{5}{Firstname}#=%=#
Author{5}{Email}#=%=#
Author{5}{Affiliation}#=%=#
Author{6}{Lastname}#=%=#
Author{6}{Firstname}#=%=#
Author{6}{Email}#=%=#
Author{6}{Affiliation}#=%=#
Author{7}{Lastname}#=%=#
Author{7}{Firstname}#=%=#
Author{7}{Email}#=%=#
Author{7}{Affiliation}#=%=#
Author{8}{Lastname}#=%=#
Author{8}{Firstname}#=%=#
Author{8}{Email}#=%=#
Author{8}{Affiliation}#=%=#
Author{9}{Lastname}#=%=#
Author{9}{Firstname}#=%=#
Author{9}{Email}#=%=#
Author{9}{Affiliation}#=%=#
Author{10}{Lastname}#=%=#
Author{10}{Firstname}#=%=#
Author{10}{Email}#=%=#
Author{10}{Affiliation}#=%=#
Author{11}{Lastname}#=%=#
Author{11}{Firstname}#=%=#
Author{11}{Email}#=%=#
Author{11}{Affiliation}#=%=#
Author{12}{Lastname}#=%=#
Author{12}{Firstname}#=%=#
Author{12}{Email}#=%=#
Author{12}{Affiliation}#=%=#
Author{13}{Lastname}#=%=#
Author{13}{Firstname}#=%=#
Author{13}{Email}#=%=#
Author{13}{Affiliation}#=%=#
Author{14}{Lastname}#=%=#
Author{14}{Firstname}#=%=#
Author{14}{Email}#=%=#
Author{14}{Affiliation}#=%=#
Author{15}{Lastname}#=%=#
Author{15}{Firstname}#=%=#
Author{15}{Email}#=%=#
Author{15}{Affiliation}#=%=#
NumberOfPages#=%=#9
CopyrightSigned#=%=#Xinhao Wang
JobTitle#==#
Organization#==#Educational Testing Service
660 Rosedale Road, Princeton, NJ 08541
Abstract#==#This paper presents and evaluates approaches to automatically score the content
correctness of spoken responses in a new language test for teachers of English
as a foreign language who are non-native speakers of English. Most existing
tests of English spoken proficiency elicit responses that are either very
constrained (e.g., reading a passage aloud) or are of a predominantly
spontaneous nature (e.g., stating an opinion on an issue). However, the
assessment discussed in this paper focuses on essential speaking skills that
English teachers need in order to be effective communicators in their
classrooms and elicits mostly responses that fall in between these extremes and
are moderately predictable. In order to automatically score the content
accuracy of these spoken responses, we propose three categories of robust
features, inspired from flexible text matching, n-grams, as well as string edit
distance metrics. The experimental results indicate that even based on speech
recognizer output, most of the feature correlations with human expert rater
scores are in the range of r = 0.4 to r = 0.5, and further, that a scoring
model for predicting human rater proficiency scores that includes our content
features can significantly outperform a baseline without these features (r =
0.56 vs. r = 0.33).
==========