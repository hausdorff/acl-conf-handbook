SubmissionNumber#=%=#160
FinalPaperTitle#=%=#Disfluency Detection Using Multi-step Stacked Learning
ShortPaperTitle#=%=#Disfluency Detection Using Multi-step Stacked Learning
Author{1}{Lastname}#=%=#Qian
Author{1}{Firstname}#=%=#Xian
Author{1}{Email}#=%=#qx@hlt.utdallas.edu
Author{1}{Affiliation}#=%=#University of Texas at Dallas
Author{2}{Lastname}#=%=#Liu
Author{2}{Firstname}#=%=#Yang
Author{2}{Email}#=%=#yangl@hlt.utdallas.edu
Author{2}{Affiliation}#=%=#
Author{3}{Lastname}#=%=#
Author{3}{Firstname}#=%=#
Author{3}{Email}#=%=#
Author{3}{Affiliation}#=%=#
Author{4}{Lastname}#=%=#
Author{4}{Firstname}#=%=#
Author{4}{Email}#=%=#
Author{4}{Affiliation}#=%=#
Author{5}{Lastname}#=%=#
Author{5}{Firstname}#=%=#
Author{5}{Email}#=%=#
Author{5}{Affiliation}#=%=#
Author{6}{Lastname}#=%=#
Author{6}{Firstname}#=%=#
Author{6}{Email}#=%=#
Author{6}{Affiliation}#=%=#
Author{7}{Lastname}#=%=#
Author{7}{Firstname}#=%=#
Author{7}{Email}#=%=#
Author{7}{Affiliation}#=%=#
Author{8}{Lastname}#=%=#
Author{8}{Firstname}#=%=#
Author{8}{Email}#=%=#
Author{8}{Affiliation}#=%=#
Author{9}{Lastname}#=%=#
Author{9}{Firstname}#=%=#
Author{9}{Email}#=%=#
Author{9}{Affiliation}#=%=#
Author{10}{Lastname}#=%=#
Author{10}{Firstname}#=%=#
Author{10}{Email}#=%=#
Author{10}{Affiliation}#=%=#
Author{11}{Lastname}#=%=#
Author{11}{Firstname}#=%=#
Author{11}{Email}#=%=#
Author{11}{Affiliation}#=%=#
Author{12}{Lastname}#=%=#
Author{12}{Firstname}#=%=#
Author{12}{Email}#=%=#
Author{12}{Affiliation}#=%=#
Author{13}{Lastname}#=%=#
Author{13}{Firstname}#=%=#
Author{13}{Email}#=%=#
Author{13}{Affiliation}#=%=#
Author{14}{Lastname}#=%=#
Author{14}{Firstname}#=%=#
Author{14}{Email}#=%=#
Author{14}{Affiliation}#=%=#
Author{15}{Lastname}#=%=#
Author{15}{Firstname}#=%=#
Author{15}{Email}#=%=#
Author{15}{Affiliation}#=%=#
NumberOfPages#=%=#6
CopyrightSigned#=%=#xian qian
JobTitle#==#
Organization#==#Computer Science Department, The University of Texas at Dallas, 800 W. Campbell Rd. Richardson, TX 75080
Abstract#==#In this paper, we propose a multi-step stacked learning model for disfluency
detection. Our method incorporates refined n-gram features step by step from
different word sequences. First, we detect filler words. Second, edited words
are detected using n-gram features extracted from both the original text and
filler filtered text. In the third step, additional n-gram features are
extracted from edit removed texts together with our newly induced in-between
features to improve edited word detection. We use Max-Margin Markov Networks
(M3Ns) as the classifier with the weighted hamming loss to balance precision
and recall. Experiments on the Switchboard corpus show that the refined n-gram
features from multiple steps and M3Ns with weighted hamming loss can
significantly improve the performance. Our method for disfluency detection
achieves the best reported F-score 0:841 without the use of additional
resources.
==========