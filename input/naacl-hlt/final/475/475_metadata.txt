SubmissionNumber#=%=#475
FinalPaperTitle#=%=#Optimal Data Set Selection: An Application to Grapheme-to-Phoneme Conversion
ShortPaperTitle#=%=#Optimal Data Set Selection: An Application to Grapheme-to-Phoneme Conversion
Author{1}{Lastname}#=%=#Kim
Author{1}{Firstname}#=%=#Young-Bum
Author{1}{Email}#=%=#ybkim@cs.wisc.edu
Author{1}{Affiliation}#=%=#University of Wisconsin-Madison
Author{2}{Lastname}#=%=#Snyder
Author{2}{Firstname}#=%=#Benjamin
Author{2}{Email}#=%=#bsnyder@cs.wisc.edu
Author{2}{Affiliation}#=%=#University of Wisconsin-Madison
Author{3}{Lastname}#=%=#
Author{3}{Firstname}#=%=#
Author{3}{Email}#=%=#
Author{3}{Affiliation}#=%=#
Author{4}{Lastname}#=%=#
Author{4}{Firstname}#=%=#
Author{4}{Email}#=%=#
Author{4}{Affiliation}#=%=#
Author{5}{Lastname}#=%=#
Author{5}{Firstname}#=%=#
Author{5}{Email}#=%=#
Author{5}{Affiliation}#=%=#
Author{6}{Lastname}#=%=#
Author{6}{Firstname}#=%=#
Author{6}{Email}#=%=#
Author{6}{Affiliation}#=%=#
Author{7}{Lastname}#=%=#
Author{7}{Firstname}#=%=#
Author{7}{Email}#=%=#
Author{7}{Affiliation}#=%=#
Author{8}{Lastname}#=%=#
Author{8}{Firstname}#=%=#
Author{8}{Email}#=%=#
Author{8}{Affiliation}#=%=#
Author{9}{Lastname}#=%=#
Author{9}{Firstname}#=%=#
Author{9}{Email}#=%=#
Author{9}{Affiliation}#=%=#
Author{10}{Lastname}#=%=#
Author{10}{Firstname}#=%=#
Author{10}{Email}#=%=#
Author{10}{Affiliation}#=%=#
Author{11}{Lastname}#=%=#
Author{11}{Firstname}#=%=#
Author{11}{Email}#=%=#
Author{11}{Affiliation}#=%=#
Author{12}{Lastname}#=%=#
Author{12}{Firstname}#=%=#
Author{12}{Email}#=%=#
Author{12}{Affiliation}#=%=#
Author{13}{Lastname}#=%=#
Author{13}{Firstname}#=%=#
Author{13}{Email}#=%=#
Author{13}{Affiliation}#=%=#
Author{14}{Lastname}#=%=#
Author{14}{Firstname}#=%=#
Author{14}{Email}#=%=#
Author{14}{Affiliation}#=%=#
Author{15}{Lastname}#=%=#
Author{15}{Firstname}#=%=#
Author{15}{Email}#=%=#
Author{15}{Affiliation}#=%=#
NumberOfPages#=%=#10
CopyrightSigned#=%=#Benjamin Snyder
JobTitle#==#
Organization#==#
Abstract#==#In this paper we introduce the task of unlabeled, optimal, data set
    selection. Given a large pool of unlabeled examples, our goal is to select
    a small subset to label, which will yield a high performance supervised
    model over the entire data set. Our first proposed method, based on the
    rank-revealing QR matrix factorization, selects a subset of words which
    span the entire word-space effectively.  For our second method, we develop
    the concept of feature coverage which we optimize with a greedy algorithm. 
We
    apply these methods to the task of grapheme-to-phoneme prediction.
    Experiments over a data-set of 8 languages show that in all scenarios,
    our selection methods are effective at yielding a small, but
    optimal set of labelled examples.  When fed into a state-of-the-art
    supervised model for grapheme-to-phoneme prediction, our methods yield
    average error reductions of 20\% over randomly selected examples.
==========