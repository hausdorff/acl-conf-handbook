SubmissionNumber#=%=#276
FinalPaperTitle#=%=#Improved Reordering for Phrase-Based Translation using Sparse Features
ShortPaperTitle#=%=#Improved Reordering for PBSMT using Sparse Features
Author{1}{Lastname}#=%=#Cherry
Author{1}{Firstname}#=%=#Colin
Author{1}{Email}#=%=#colin.cherry@nrc.gc.ca
Author{1}{Affiliation}#=%=#NRC
Author{2}{Lastname}#=%=#
Author{2}{Firstname}#=%=#
Author{2}{Email}#=%=#
Author{2}{Affiliation}#=%=#
Author{3}{Lastname}#=%=#
Author{3}{Firstname}#=%=#
Author{3}{Email}#=%=#
Author{3}{Affiliation}#=%=#
Author{4}{Lastname}#=%=#
Author{4}{Firstname}#=%=#
Author{4}{Email}#=%=#
Author{4}{Affiliation}#=%=#
Author{5}{Lastname}#=%=#
Author{5}{Firstname}#=%=#
Author{5}{Email}#=%=#
Author{5}{Affiliation}#=%=#
Author{6}{Lastname}#=%=#
Author{6}{Firstname}#=%=#
Author{6}{Email}#=%=#
Author{6}{Affiliation}#=%=#
Author{7}{Lastname}#=%=#
Author{7}{Firstname}#=%=#
Author{7}{Email}#=%=#
Author{7}{Affiliation}#=%=#
Author{8}{Lastname}#=%=#
Author{8}{Firstname}#=%=#
Author{8}{Email}#=%=#
Author{8}{Affiliation}#=%=#
Author{9}{Lastname}#=%=#
Author{9}{Firstname}#=%=#
Author{9}{Email}#=%=#
Author{9}{Affiliation}#=%=#
Author{10}{Lastname}#=%=#
Author{10}{Firstname}#=%=#
Author{10}{Email}#=%=#
Author{10}{Affiliation}#=%=#
Author{11}{Lastname}#=%=#
Author{11}{Firstname}#=%=#
Author{11}{Email}#=%=#
Author{11}{Affiliation}#=%=#
Author{12}{Lastname}#=%=#
Author{12}{Firstname}#=%=#
Author{12}{Email}#=%=#
Author{12}{Affiliation}#=%=#
Author{13}{Lastname}#=%=#
Author{13}{Firstname}#=%=#
Author{13}{Email}#=%=#
Author{13}{Affiliation}#=%=#
Author{14}{Lastname}#=%=#
Author{14}{Firstname}#=%=#
Author{14}{Email}#=%=#
Author{14}{Affiliation}#=%=#
Author{15}{Lastname}#=%=#
Author{15}{Firstname}#=%=#
Author{15}{Email}#=%=#
Author{15}{Affiliation}#=%=#
NumberOfPages#=%=#10
CopyrightSigned#=%=#NA
JobTitle#==#
Organization#==#
Abstract#==#There have been many recent investigations into methods to tune SMT systems
using large numbers of sparse features. However, there have not been nearly so
many examples of helpful sparse features, especially for phrase-based systems.
We use sparse features to address reordering, which is often considered a weak
point of phrase-based translation. Using a hierarchical reordering model as our
baseline, we show that simple features coupling phrase orientation to frequent
words or word-clusters can improve translation quality, with boosts of up to
1.2 BLEU points in Chinese-English and 1.8 in Arabic-English. We compare this
solution to a more traditional maximum entropy approach, where a probability
model with similar features is trained on word-aligned bitext. We show that
sparse decoder features outperform maximum entropy handily, indicating that
there are major advantages to optimizing reordering features directly for BLEU
with the decoder in the loop.
==========