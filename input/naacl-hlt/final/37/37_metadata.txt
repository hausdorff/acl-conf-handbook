SubmissionNumber#=%=#37
FinalPaperTitle#=%=#Linguistic Regularities in Continuous Space Word Representations
ShortPaperTitle#=%=#Linguistic Regularities in Continuous Space Word Representations
Author{1}{Lastname}#=%=#Mikolov
Author{1}{Firstname}#=%=#Tomas
Author{1}{Email}#=%=#tmikolov@gmail.com
Author{1}{Affiliation}#=%=#Google
Author{2}{Lastname}#=%=#Yih
Author{2}{Firstname}#=%=#Wen-tau
Author{2}{Email}#=%=#scottyih@microsoft.com
Author{2}{Affiliation}#=%=#Microsoft Research
Author{3}{Lastname}#=%=#Zweig
Author{3}{Firstname}#=%=#Geoffrey
Author{3}{Email}#=%=#gzweig@microsoft.com
Author{3}{Affiliation}#=%=#Microsoft Research
Author{4}{Lastname}#=%=#
Author{4}{Firstname}#=%=#
Author{4}{Email}#=%=#
Author{4}{Affiliation}#=%=#
Author{5}{Lastname}#=%=#
Author{5}{Firstname}#=%=#
Author{5}{Email}#=%=#
Author{5}{Affiliation}#=%=#
Author{6}{Lastname}#=%=#
Author{6}{Firstname}#=%=#
Author{6}{Email}#=%=#
Author{6}{Affiliation}#=%=#
Author{7}{Lastname}#=%=#
Author{7}{Firstname}#=%=#
Author{7}{Email}#=%=#
Author{7}{Affiliation}#=%=#
Author{8}{Lastname}#=%=#
Author{8}{Firstname}#=%=#
Author{8}{Email}#=%=#
Author{8}{Affiliation}#=%=#
Author{9}{Lastname}#=%=#
Author{9}{Firstname}#=%=#
Author{9}{Email}#=%=#
Author{9}{Affiliation}#=%=#
Author{10}{Lastname}#=%=#
Author{10}{Firstname}#=%=#
Author{10}{Email}#=%=#
Author{10}{Affiliation}#=%=#
Author{11}{Lastname}#=%=#
Author{11}{Firstname}#=%=#
Author{11}{Email}#=%=#
Author{11}{Affiliation}#=%=#
Author{12}{Lastname}#=%=#
Author{12}{Firstname}#=%=#
Author{12}{Email}#=%=#
Author{12}{Affiliation}#=%=#
Author{13}{Lastname}#=%=#
Author{13}{Firstname}#=%=#
Author{13}{Email}#=%=#
Author{13}{Affiliation}#=%=#
Author{14}{Lastname}#=%=#
Author{14}{Firstname}#=%=#
Author{14}{Email}#=%=#
Author{14}{Affiliation}#=%=#
Author{15}{Lastname}#=%=#
Author{15}{Firstname}#=%=#
Author{15}{Email}#=%=#
Author{15}{Affiliation}#=%=#
NumberOfPages#=%=#6
CopyrightSigned#=%=#Geofrey Zweig
JobTitle#==#
Organization#==#Microsoft Corp.
One Microsoft Way
Redmond, WA 98052
Abstract#==#Continuous space language models have recently demonstrated outstanding results
across a variety of tasks. In this paper, we examine the vector-space word
representations that are implicitly learned by the input-layer weights. We find
that these representations are surprisingly good at capturing syntactic
and semantic regularities in language, and that each relationship is
characterized by a relation-specific vector offset. This allows vector-oriented
reasoning based on the offsets between words. For example, the male/female
relationship is automatically learned, and with the induced vector
representations, “King - Man + Woman” results in a vector very close to
“Queen.” We demonstrate that the word vectors capture syntactic
regularities by means of syntactic analogy questions (provided with this
paper), and are able to correctly answer almost 40% of the questions. We
demonstrate that the word vectors capture semantic regularities by using the
vector offset method to answer SemEval-2012 Task 2 questions. Remarkably, this
method outperforms the best previous systems.
==========