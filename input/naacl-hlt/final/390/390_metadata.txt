SubmissionNumber#=%=#390
FinalPaperTitle#=%=#Minibatch and Parallelization for Online Large Margin Structured Learning
ShortPaperTitle#=%=#Minibatch and Parallelization for Online Large Margin Structured Learning
Author{1}{Lastname}#=%=#Zhao
Author{1}{Firstname}#=%=#Kai
Author{1}{Email}#=%=#z.kaayy@gmail.com
Author{1}{Affiliation}#=%=#City University of New York (CUNY)
Author{2}{Lastname}#=%=#Huang
Author{2}{Firstname}#=%=#Liang
Author{2}{Email}#=%=#liang.huang.sh@gmail.com
Author{2}{Affiliation}#=%=#City University of New York (CUNY)
Author{3}{Lastname}#=%=#
Author{3}{Firstname}#=%=#
Author{3}{Email}#=%=#
Author{3}{Affiliation}#=%=#
Author{4}{Lastname}#=%=#
Author{4}{Firstname}#=%=#
Author{4}{Email}#=%=#
Author{4}{Affiliation}#=%=#
Author{5}{Lastname}#=%=#
Author{5}{Firstname}#=%=#
Author{5}{Email}#=%=#
Author{5}{Affiliation}#=%=#
Author{6}{Lastname}#=%=#
Author{6}{Firstname}#=%=#
Author{6}{Email}#=%=#
Author{6}{Affiliation}#=%=#
Author{7}{Lastname}#=%=#
Author{7}{Firstname}#=%=#
Author{7}{Email}#=%=#
Author{7}{Affiliation}#=%=#
Author{8}{Lastname}#=%=#
Author{8}{Firstname}#=%=#
Author{8}{Email}#=%=#
Author{8}{Affiliation}#=%=#
Author{9}{Lastname}#=%=#
Author{9}{Firstname}#=%=#
Author{9}{Email}#=%=#
Author{9}{Affiliation}#=%=#
Author{10}{Lastname}#=%=#
Author{10}{Firstname}#=%=#
Author{10}{Email}#=%=#
Author{10}{Affiliation}#=%=#
Author{11}{Lastname}#=%=#
Author{11}{Firstname}#=%=#
Author{11}{Email}#=%=#
Author{11}{Affiliation}#=%=#
Author{12}{Lastname}#=%=#
Author{12}{Firstname}#=%=#
Author{12}{Email}#=%=#
Author{12}{Affiliation}#=%=#
Author{13}{Lastname}#=%=#
Author{13}{Firstname}#=%=#
Author{13}{Email}#=%=#
Author{13}{Affiliation}#=%=#
Author{14}{Lastname}#=%=#
Author{14}{Firstname}#=%=#
Author{14}{Email}#=%=#
Author{14}{Affiliation}#=%=#
Author{15}{Lastname}#=%=#
Author{15}{Firstname}#=%=#
Author{15}{Email}#=%=#
Author{15}{Affiliation}#=%=#
NumberOfPages#=%=#10
CopyrightSigned#=%=#Liang Huang
JobTitle#==#
Organization#==#65-30 Kissena Blvd, SB A2-2
CS Dept, Queens College, CUNY
Flushing, NY 11367
Abstract#==#Online learning algorithms such as perceptron and MIRA have become popular for
many NLP tasks thanks to their simpler architect-ure and faster convergence
over batch learning methods. However, while batch learning such as CRF is
easily parallelizable, online learning is much harder to parallelize: previous
efforts often witness a decrease in the converged accuracy, and the speedup is
typically very small (∼3) even with many (10+) processors. We instead present
a much simpler architecture based on “mini-batches”, which is trivially
parallelizable. We show that, unlike previous methods, minibatch learning (in
serial mode) actually improves the converged accuracy for both perceptron and
MIRA learning, and when combined with simple parallelization, minibatch leads
to very significant speedups (up to 9x on 12 processors) on state-of-the-art
parsing and tagging systems.
==========