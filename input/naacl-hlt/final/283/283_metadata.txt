SubmissionNumber#=%=#283
FinalPaperTitle#=%=#Overcoming the Memory Bottleneck in Distributed Training of Latent Variable Models of Text
ShortPaperTitle#=%=#Overcoming the Memory Bottleneck in Distributed Training of Latent Variable Models of Text
Author{1}{Lastname}#=%=#Yang
Author{1}{Firstname}#=%=#Yi
Author{1}{Email}#=%=#yiyang@u.northwestern.edu
Author{1}{Affiliation}#=%=#Northwestern University
Author{2}{Lastname}#=%=#Yates
Author{2}{Firstname}#=%=#Alexander
Author{2}{Email}#=%=#alexander.yates@temple.edu
Author{2}{Affiliation}#=%=#Temple University
Author{3}{Lastname}#=%=#Downey
Author{3}{Firstname}#=%=#Doug
Author{3}{Email}#=%=#ddowney@eecs.northwestern.edu
Author{3}{Affiliation}#=%=#Northwestern University
Author{4}{Lastname}#=%=#
Author{4}{Firstname}#=%=#
Author{4}{Email}#=%=#
Author{4}{Affiliation}#=%=#
Author{5}{Lastname}#=%=#
Author{5}{Firstname}#=%=#
Author{5}{Email}#=%=#
Author{5}{Affiliation}#=%=#
Author{6}{Lastname}#=%=#
Author{6}{Firstname}#=%=#
Author{6}{Email}#=%=#
Author{6}{Affiliation}#=%=#
Author{7}{Lastname}#=%=#
Author{7}{Firstname}#=%=#
Author{7}{Email}#=%=#
Author{7}{Affiliation}#=%=#
Author{8}{Lastname}#=%=#
Author{8}{Firstname}#=%=#
Author{8}{Email}#=%=#
Author{8}{Affiliation}#=%=#
Author{9}{Lastname}#=%=#
Author{9}{Firstname}#=%=#
Author{9}{Email}#=%=#
Author{9}{Affiliation}#=%=#
Author{10}{Lastname}#=%=#
Author{10}{Firstname}#=%=#
Author{10}{Email}#=%=#
Author{10}{Affiliation}#=%=#
Author{11}{Lastname}#=%=#
Author{11}{Firstname}#=%=#
Author{11}{Email}#=%=#
Author{11}{Affiliation}#=%=#
Author{12}{Lastname}#=%=#
Author{12}{Firstname}#=%=#
Author{12}{Email}#=%=#
Author{12}{Affiliation}#=%=#
Author{13}{Lastname}#=%=#
Author{13}{Firstname}#=%=#
Author{13}{Email}#=%=#
Author{13}{Affiliation}#=%=#
Author{14}{Lastname}#=%=#
Author{14}{Firstname}#=%=#
Author{14}{Email}#=%=#
Author{14}{Affiliation}#=%=#
Author{15}{Lastname}#=%=#
Author{15}{Firstname}#=%=#
Author{15}{Email}#=%=#
Author{15}{Affiliation}#=%=#
NumberOfPages#=%=#6
CopyrightSigned#=%=#YI YANG
JobTitle#==#
Organization#==#Northwestern University  Evanston, IL
Abstract#==#Large unsupervised latent variable models (LVMs) of text, such as Latent
Dirichlet Allocation models or Hidden Markov Models (HMMs), are constructed
using parallel training algorithms on computational clusters. The memory
required to hold LVM parameters forms a bottleneck in training more powerful
models.  In this paper, we show how the memory required for parallel LVM
training can be reduced by partitioning the training corpus to minimize the
number of unique words on any computational node. We present a greedy
document partitioning technique for the task. For large corpora, our approach
reduces memory consumption by over 50%, and trains the same models up to three
times faster, when compared with existing approaches for parallel LVM training.
==========