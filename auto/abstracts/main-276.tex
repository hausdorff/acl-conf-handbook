There have been many recent investigations into methods to tune SMT systems using large numbers of sparse features. However, there have not been nearly so
 many examples of helpful sparse features, especially for phrase-based systems.
 We use sparse features to address reordering, which is often considered a weak
 point of phrase-based translation. Using a hierarchical reordering model as our
 baseline, we show that simple features coupling phrase orientation to frequent
 words or word-clusters can improve translation quality, with boosts of up to
 1.2 BLEU points in Chinese-English and 1.8 in Arabic-English. We compare this
 solution to a more traditional maximum entropy approach, where a probability
 model with similar features is trained on word-aligned bitext. We show that
 sparse decoder features outperform maximum entropy handily, indicating that
 there are major advantages to optimizing reordering features directly for BLEU
 with the decoder in the loop.

