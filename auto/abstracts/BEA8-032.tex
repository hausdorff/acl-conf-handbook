Our efforts in the 2013 NLI shared task focused on the potential benefits of external corpora. We show that including training data from multiple corpora is
 highly effective at robust, cross-corpus NLI (i.e. open-training task 1),
 particularly when some form of domain adaptation is also applied. This method
 can also be used to boost performance even when training data from the same
 corpus is available (i.e. open-training task 2). However, in the
 closed-training task, despite testing a number of new features, we did not see
 much improvement on a simple model based on earlier work.

