In this paper we discuss our participation to the 2013 Semeval Semantic Textual Similarity task. Our core features include (i) a set of metrics borrowed from
 automatic machine translation, originally intended to evaluate automatic
 against reference translations and (ii) an instance of explicit semantic
 analysis, built
 upon opening paragraphs of Wikipedia 2010 articles. Our similarity estimator
 relies on a support vector regressor with RBF kernel. Our best approach
 required 13 machine translation metrics + explicit semantic analysis and ranked
 65 in the competition. Our post-competition analysis shows that the features
 have a good expression level, but overfitting and —mainly— normalization
 issues caused
 our correlation values to decrease.

