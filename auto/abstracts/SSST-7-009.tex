We show that combining both bottom-up rule chunking and top-down rule segmentation search strategies in purely unsupervised learning of phrasal
 inversion transduction grammars yields significantly better translation
 accuracy than either strategy alone. Previous approaches have relied on
 incrementally building larger rules by chunking smaller rules bottom-up; we
 introduce a complementary top-down model that incrementally builds shorter
 rules by segmenting larger rules. Specifically, we combine iteratively chunked
 rules from Saers et al. (2012) with our new iteratively segmented rules. These
 integrate seamlessly because both stay strictly within a pure transduction
 grammar framework inducing under matching models during both training and
 testing---instead of decoding under a completely different model architecture
 than what is assumed during the training phases, which violates an elementary
 principle of machine learning and statistics. To be able to drive induction
 top-down, we introduce a minimum description length objective that trades off
 maximum likelihood against model size. We show empirically that combining the
 more liberal rule chunking model with a more conservative rule segmentation
 model results in significantly better translations than either strategy in
 isolation.

