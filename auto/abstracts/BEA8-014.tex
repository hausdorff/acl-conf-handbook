We present an experiment aimed at improving interpretation robustness a tutorial dialogue system that relies on detailed semantic interpretation and
 dynamic natural language feedback generation. We show that we can improve
 overall interpretation quality by combining the output of a semantic
 interpreter with that of a statistical classifier trained on the subset of
 student utterances where semantic interpretation fails. This improves on a
 previous result which used a similar approach but trained the classifier on a
 substantially larger data set containing all student utterances.  Finally, we
 discuss how the labels from the classifier can be integrated effectively with
 the dialogue system's existing error recovery policies.

