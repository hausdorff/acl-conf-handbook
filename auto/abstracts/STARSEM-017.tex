Short answer questions for reading comprehension are a common task   in foreign language learning. Automatic short answer scoring is the
   task of automatically assessing the semantic content of a student's
   answer, marking it e.g. as correct or incorrect. While previous
   approaches mainly focused on comparing a learner answer to some
   reference answer provided by the teacher, we explore the use of the
   underlying reading texts as additional evidence for the
   classification. First, we conduct a corpus study targeting the
   relation between answers and sentences in reading texts for learners
   of German. Second, we use the reading text directly for
   classification, considering three different models: an answer-based
   classifier extended with textual features, a simple text-based
   classifier, and a model that combines the two according to
   confidence of the text-based classification. The most promising
   approach is the first one, results for which show that textual
   features improve classification accuracy. While the other two models
   do not improve classification accuracy, they do investigate the role
   of the text and suggest possibilities for developing automatic
   answer scoring systems with less supervision needed from
   instructors.

