Automatic scoring of short text responses to educational assessment items is a challenging task, particularly because large amounts of labeled data (i.e.,
 human-scored responses) may or may not be available due to the variety of
 possible questions and topics.               As such, it seems desirable to integrate
 various approaches, making use of model answers from experts (e.g., to give
 higher scores to responses that are similar), prescored student responses
 (e.g., to learn direct associations between particular phrases and scores),
 etc. Here, we describe a system that uses stacking (Wolpert, 1992) and domain
 adaptation (Daume II, 2007) to achieve this aim, allowing us to integrate
 item-specific $n$-gram features and more general text similarity measures
 (Heilman and Madnani, 2012). We report encouraging results from the Joint
 Student Response Analysis and 8th Recognizing Textual Entailment Challenge.

