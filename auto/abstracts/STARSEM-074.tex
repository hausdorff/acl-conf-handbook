Semantic Textual Similarity (STS) measures the degree of semantic equivalence, on a scale from 0 to 5. This year we set up two tasks. The core task is similar
 to STS 2012, with pairs of sentences from news headlines, ma- chine translation
 evaluation datasets and lexi- cal resource glosses. The typed-similarity task
 is novel and involves pairs of cultural heritage items which are described with
 metadata like title, author or description. Several types of similarity have
 been defined, including similar author, similar time period or similar
 location. The annotation leverages crowdsourcing, with relative high
 inter-annotator correlation, rang- ing from 62\% to 87\%. The core task attracted
 34 participants corresponding to 89 runs, and the typed-similarity task
 attracted 6 teams cor- responding to 14 runs.

