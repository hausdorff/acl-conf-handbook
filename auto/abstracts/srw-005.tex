In this paper we revisit the task of quantitative evaluation of coreference resolution systems. We review the most used metrics (MUC, B-CUBED, CEAF and
 BLANC) on basis of their evaluation of coreference resolution in five texts
 from the OntoNotes corpus. We examine both correlation between the metrics and
 the degree to which our human judgement of coreference resolution agrees with
 the metrics. In conclusion we claim that loss of information value is an
 essential factor in human perception of the degree of success or failure of
 coreference resolution and that including a layer of mention information weight
 could improve both the evaluation and coreference resolution itself.

